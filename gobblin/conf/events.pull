job.name=events
job.group=events
job.description=events
job.lock.enabled=true
job.runonce=true

kafka.brokers=${env:KAFKA_BROKERS}
kafka.workunit.packer.type=BI_LEVEL

source.class=org.apache.gobblin.source.extractor.extract.kafka.KafkaSimpleSource
extract.namespace=gobblin.extract.kafka

converter.classes=com.tfgco.eventsgateway.gobblin.EventBytesToAvroConverter

writer.partitioner.class=com.tfgco.eventsgateway.gobblin.GobblinDailyPartitioner
writer.partition.pattern=YYYY/MM/dd
writer.partition.columns=timestamp
writer.builder.class=org.apache.gobblin.writer.AvroDataWriterBuilder
writer.file.path.type=tablename
writer.partition.timezone=UTC
writer.destination.type=HDFS
writer.output.format=AVRO

writer.codec.type=SNAPPY
writer.deflate.level=6
writer.include.partition.in.file.names=false

mr.job.max.mappers=5

extract.limit.enabled=true
extract.limit.type=time
extract.limit.timeLimit=10
extract.limit.timeLimitTimeunit=minutes

topic.whitelist=sv\-uploads\-.*
data.publisher.type=org.apache.gobblin.publisher.TimePartitionedDataPublisher

metrics.reporting.file.enabled=false
metrics.log.dir=${env:GOBBLIN_WORK_DIR}/metrics
metrics.reporting.file.suffix=txt

bootstrap.with.offset=earliest

email.alert.enabled=false
email.host=${env:EMAIL_HOST}
email.smtp.port=25
email.user=${env:EMAIL_USER}
email.password=${env:EMAIL_PASSWORD}
email.from=${env.EMAIL_FROM}
email.tos=${env.EMAIL_TOS}
